{"cells":[{"cell_type":"markdown","metadata":{"id":"UChknajcavFm"},"source":["# **Spam Detection**\n","\n","## Spam Message Classification with Spark NLP\n","\n","In this notebook, you'll build an end-to-end **SMS spam detection pipeline** using existing pretrained models with [Spark NLP](https://sparknlp.org/) and [Apache Spark](https://spark.apache.org/).\n","\n","You‚Äôll learn how to:\n","\n","- Set up a Spark NLP pipeline in Python\n","- Use pretrained models (Universal Sentence Encoder and a spam classifier)\n","- Build and run a text classification workflow\n","- Visualize predictions made on sample SMS texts\n","\n","By the end, you‚Äôll understand how to integrate Spark NLP into scalable NLP projects.\n","\n","### Compatibility\n","\n","| Platform                     | Compatible | Recommended | Notes                                                                                                             |\n","| ---------------------------- | ---------- | ----------- | ----------------------------------------------------------------------------------------------------------------- |\n","| **Local (e.g., M1 MacBook)** | ‚úÖ Yes     | ‚úÖ Yes      | -                                                                                                                 |\n","| **Google Colab**             | ‚úÖ Yes     | ‚úÖ Yes      | -                                                                                                                 |\n","| **Midway3 Login Node**       | ‚úÖ Yes     | ‚ùå No       | It is generally not recommended to run Spark jobs on the login nodes.                                             |\n","| **Midway3 Compute Node**     | ‚úÖ Yes     | ‚úÖ Yes      | Use with `sinteractive`, `scode` or [Open OnDemand](https://midway3-ondemand.rcc.uchicago.edu/) (Jupyter/VSCode). |\n","\n","### Credits\n","\n","Adapted from [JohnSnowLabs Spark NLP SMS Spam Classifier Example](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/CLASSIFICATION_EN_SPAM.ipynb)\n"]},{"cell_type":"markdown","metadata":{"id":"n_Zz-kwQa7Ez"},"source":["## 1. Environment Setup\n"]},{"cell_type":"markdown","metadata":{"id":"eqVqmsNz8tNb"},"source":["This notebook is tested to run with Python 3.12.11.\n","\n","Please **run the following command on the the login node** to install the required packages if you haven't done so already.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"sIJfXkK54WFk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758553523704,"user_tz":300,"elapsed":70706,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}},"outputId":"a2fdb890-818f-4018-b728-a7821cc175b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark==3.5.6\n","  Downloading pyspark-3.5.6.tar.gz (317.4 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m317.4/317.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting spark-nlp==6.0.5\n","  Downloading spark_nlp-6.0.5-py2.py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark==3.5.6) (0.10.9.7)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Downloading spark_nlp-6.0.5-py2.py3-none-any.whl (718 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m718.9/718.9 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.6-py2.py3-none-any.whl size=317895798 sha256=d241445b456fe54367ac1c48857d25ae25dae1b00beb5e8710ff3399c055b6e3\n","  Stored in directory: /root/.cache/pip/wheels/64/62/f3/ec15656ea4ada0523cae62a1827fe7beb55d3c8c87174aad4a\n","Successfully built pyspark\n","Installing collected packages: spark-nlp, pyspark\n","  Attempting uninstall: pyspark\n","    Found existing installation: pyspark 3.5.1\n","    Uninstalling pyspark-3.5.1:\n","      Successfully uninstalled pyspark-3.5.1\n","Successfully installed pyspark-3.5.6 spark-nlp-6.0.5\n"]}],"source":["# Install PySpark and Spark NLP\n","# If you are on Midway, run this command in the activated python environment on rcc.midway3, not in this command line\n","%pip install pyspark==3.5.6 spark-nlp==6.0.5 pandas numpy matplotlib seaborn"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1804,"status":"ok","timestamp":1758553525525,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"v29AZ9XO5AhU"},"outputs":[],"source":["import os\n","import json\n","import pandas as pd\n","import numpy as np\n","\n","import sparknlp\n","import pyspark.sql.functions as F\n","\n","from pyspark.ml import Pipeline\n","from pyspark.sql import SparkSession\n","from sparknlp.annotator import *\n","from sparknlp.base import *\n","from sparknlp.pretrained import PretrainedPipeline\n","from pyspark.sql.types import StringType, IntegerType"]},{"cell_type":"markdown","metadata":{"id":"pqorlWy9a9pF"},"source":["## 2. Start Spark Session\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":254},"executionInfo":{"elapsed":78775,"status":"ok","timestamp":1758553604297,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"sI-CZ9PO5GW9","outputId":"666e9c2c-a1ef-40d4-e0ab-f34a1cb4505c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Spark NLP version 6.0.5\n","Apache Spark version: 3.5.6\n"]},{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x787553b711c0>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://514d5c72b78d:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.6</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Spark NLP</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":3}],"source":["spark = sparknlp.start()\n","\n","print(\"Spark NLP version\", sparknlp.version())\n","print(\"Apache Spark version:\", spark.version)\n","\n","spark"]},{"cell_type":"markdown","metadata":{"id":"0JaFJmC_bD04"},"source":["## 3. Pretained Models\n"]},{"cell_type":"markdown","metadata":{"id":"qoNG-A5v8tNd"},"source":["### SparkNLP Models Hub\n","\n","Spark NLP provides a **Models Hub** containing thousands of pretrained models and pipelines:\n","\n","- Explore the hub: [sparknlp.org/models](https://sparknlp.org/models)\n","\n","Even though these models are usually not the latest SOTA (as is usually found on HuggingFace), they are **production-ready** and can be used as-is or fine-tuned for specific tasks.\n","\n","### Models used in this notebook\n","\n","- **Universal Sentence Encoder**: \"tfhub_use\"  \n","  A bridge to TensorFlow Hub's USE model  \n","  Model page: [sparknlp.org/2020/04/17/tfhub_use.html](https://sparknlp.org/2020/04/17/tfhub_use.html)\n","\n","- **Spam Classifier**: \"classifierdl_use_spam\"  \n","  Classifies SMS as `spam` or `ham`  \n","  Model page: [sparknlp.org/2021/01/09/classifierdl_use_spam_en.html](https://sparknlp.org/2021/01/09/classifierdl_use_spam_en.html)\n","\n","The easiest way to load these models is to use the `pretrained()` method, which automatically downloads the model and its dependencies (if not already cached) and loads it into your Spark NLP pipeline.\n","\n","```python\n","UniversalSentenceEncoder.pretrained(\"tfhub_use\", \"en\")\n","ClassifierDLModel.pretrained(\"classifierdl_use_spam\", \"en\")\n","```\n","\n","However, since we will be also working with Midway3, where **compute node internet access is disabled**, we will need to download the models to a directory on login nodes and load them from there.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1758553604316,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"Y6s6ljDsH9ZK"},"outputs":[],"source":["### Select Model\n","model_name = \"classifierdl_use_spam\""]},{"cell_type":"markdown","metadata":{"id":"YjC6XyHf8tNd"},"source":["The models used in this notebook are already downloaded to the course directory.\n","\n","But if you want to download them to your personal directory, please uncomment the following cell and run it.\n","\n","If you choose to download the models to your personal directory, you will need to change the `MODEL_DIR` variable in the next section to point to the `~/cache_pretrained/` folder where the models are downloaded to by default.\n","\n","Usually the S3 URIs of the models can be found on their model pages.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ateWev538tNd"},"outputs":[],"source":["# from sparknlp.pretrained import ResourceDownloader\n","\n","# # List of (model_name, language) pairs used in your pipeline\n","# MODELS = [\n","#     \"s3://auxdata.johnsnowlabs.com/public/models/tfhub_use_en_2.4.0_2.4_1587136330099.zip\",            # Universal Sentence Encoder\n","#     \"s3://auxdata.johnsnowlabs.com/public/models/classifierdl_use_spam_en_2.7.1_2.4_1610187019592.zip\" # SMS spam/ham classifier\n","# ]\n","\n","# # Download loop\n","# for model_path in MODELS:\n","#     print(f\"Downloading {model_path} ‚Ä¶\")\n","#     ResourceDownloader.downloadModelDirectly(model_path)\n","\n","# print(\"All requested models are now cached locally\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_ySDxHI8tNe"},"outputs":[],"source":["# Run this if you are on Midway\n","# Set the model directory to where the models are saved\n","MODEL_DIR = \"../models/\"\n","\n","use_path = os.path.join(MODEL_DIR, \"tfhub_use_en_2.4.0_2.4_1587136330099\")\n","classifier_path = os.path.join(\n","    MODEL_DIR, \"classifierdl_use_spam_en_2.7.1_2.4_1610187019592\"\n",")"]},{"cell_type":"markdown","metadata":{"id":"aGttu2LqbAIn"},"source":["## 4. Example Data\n"]},{"cell_type":"markdown","metadata":{"id":"sYFNOwmf8tNe"},"source":["We‚Äôll use a few hand-labeled SMS messages to evaluate spam classification.\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1758553872620,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"WGFzBK1EX8wm"},"outputs":[],"source":["text_list = [\n","    \"Hiya do u like the hlday pics looked horrible in them so took mo out! Hows the camp Amrca thing? Speak soon Serena:)\",  # HAM\n","    \"U have a secret admirer who is looking 2 make contact with U-find out who they R*reveal who thinks UR so special-call on 09058094594\",  # SPAM\n","    \"Hey, just checking in. How was the exam? Let me know when you're free to catch up.\",  # HAM\n","    \"Congratulations! You've won a ¬£1000 Tesco gift card. To claim, text WIN to 80062. Hurry, offer ends soon!\",  # SPAM\n","    \"Dinner's at 7. Don't be late again üòÑ Mum's making lasagna!\",  # HAM\n","    \"You‚Äôve been selected for a guaranteed cash prize of ¬£2000! To claim, call 09061701461 NOW!\",  # SPAM\n","    \"Got to the hotel safely. Weather‚Äôs great. Wish you were here!\",  # HAM\n","    \"FreeMsg: CLAIM YOUR FREE RINGTONE NOW! Just text the word ‚ÄòTONE‚Äô to 87131. Don‚Äôt miss out!\",  # SPAM\n","    \"Happy birthday! üéâ Hope today is full of good vibes and cake.\",  # HAM\n","    \"URGENT! Your mobile number has won ¬£5000 cash. Call 09066362231 to claim your prize. T&C apply.\",  # SPAM\n","]"]},{"cell_type":"markdown","metadata":{"id":"BcE65Pc0bGPO"},"source":["## 5. Define Spark NLP pipeline\n"]},{"cell_type":"markdown","metadata":{"id":"4w6kddXv8tNe"},"source":["It‚Äôs a common practice in **PySpark** (which we will touch on in wk1.3-netflix-challenge) and **Spark NLP** to build reusable, modular pipelines using `Pipeline` and its components.\n","\n","For more information on building custom pipelines in Spark NLP:\n","[Custom Pipelines Guide](https://sparknlp.org/api/python/user_guide/custom_pipelines.html)\n","\n","Below are the key components used in this SMS spam classification example:\n","\n","| Class                                         | Description                                                                                                                                                                                   |\n","| --------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n","| `sparknlp.base.DocumentAssembler`             | The entry point for any Spark NLP pipeline. It transforms raw text into `document` type, which downstream annotators can consume. All NLP pipelines begin with this step.                     |\n","| `sparknlp.annotator.UniversalSentenceEncoder` | A Spark NLP wrapper for TensorFlow Hub‚Äôs Universal Sentence Encoder (USE). Converts text into dense vector embeddings suitable for classification and semantic similarity.                    |\n","| `sparknlp.annotator.ClassifierDLModel`        | A deep learning-based text classifier. Consumes sentence/document embeddings and predicts a class label. Trained on top of embeddings like USE, BERT, etc.                                    |\n","| `pyspark.ml.Pipeline`                         | A core Spark ML class used to chain multiple transformers and estimators into a single pipeline. Used for both training (fit) and inference (transform). Enables a clean and scalable design. |\n"]},{"cell_type":"code","source":["import os\n","import urllib.request\n","\n","def has_internet(timeout: int = 3) -> bool:\n","    try:\n","        urllib.request.urlopen(\"https://clients3.google.com/generate_204\", timeout=timeout)\n","        return True\n","    except Exception:\n","        return False\n","\n","online_ok = has_internet()\n","print(f\"Internet available: {online_ok}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ocMSAFoJ9s7E","executionInfo":{"status":"ok","timestamp":1758553655742,"user_tz":300,"elapsed":68,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}},"outputId":"a289d8c2-51d5-4ace-977b-9f8ec7e9e3bf"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Internet available: True\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":113829,"status":"ok","timestamp":1758553770867,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"K2CS_jdi5Phc","outputId":"0fd1ef9e-f7e7-430e-ade1-edc73f1e6cf7"},"outputs":[{"output_type":"stream","name":"stdout","text":["tfhub_use download started this may take some time.\n","Approximate size to download 923.7 MB\n","[OK!]\n","classifierdl_use_spam download started this may take some time.\n","Approximate size to download 21.3 MB\n","[OK!]\n"]}],"source":["# Build Data Assembly\n","documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n","\n","# Load models: online if possible; otherwise local paths\n","if online_ok:\n","    use = UniversalSentenceEncoder.pretrained(\"tfhub_use\", \"en\")\n","    document_classifier = ClassifierDLModel.pretrained(\"classifierdl_use_spam\", \"en\")\n","else:\n","    use = UniversalSentenceEncoder.load(use_path)\n","    document_classifier = ClassifierDLModel.load(classifier_path)\n","\n","# Set input and output columns\n","use = use.setInputCols([\"document\"]).setOutputCol(\"sentence_embeddings\")\n","document_classifier = (\n","    document_classifier\n","    .setInputCols([\"sentence_embeddings\"])\n","    .setOutputCol(\"class_\")\n",")\n","\n","# Build the pipeline\n","nlpPipeline = Pipeline(stages=[documentAssembler, use, document_classifier])"]},{"cell_type":"markdown","metadata":{"id":"n1wPffwfbKNb"},"source":["## 6. Run the pipeline\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":7335,"status":"ok","timestamp":1758553881447,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"RZxkeqNpbR02"},"outputs":[],"source":["df = spark.createDataFrame(text_list, StringType()).toDF(\"text\")\n","result = nlpPipeline.fit(df).transform(df)"]},{"cell_type":"markdown","metadata":{"id":"7nvJc6dwbX9X"},"source":["## 7. Visualize results\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10735,"status":"ok","timestamp":1758553892185,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"},"user_tz":300},"id":"P84W1Z4uPI_b","outputId":"a2a7e99c-013d-4695-b030-17dec9694fa7"},"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------------------------------------------------------------------------------------------------------------------+-----+\n","|document                                                                                                                            |class|\n","+------------------------------------------------------------------------------------------------------------------------------------+-----+\n","|Hiya do u like the hlday pics looked horrible in them so took mo out! Hows the camp Amrca thing? Speak soon Serena:)                |ham  |\n","|U have a secret admirer who is looking 2 make contact with U-find out who they R*reveal who thinks UR so special-call on 09058094594|ham  |\n","|Hey, just checking in. How was the exam? Let me know when you're free to catch up.                                                  |ham  |\n","|Congratulations! You've won a ¬£1000 Tesco gift card. To claim, text WIN to 80062. Hurry, offer ends soon!                           |spam |\n","|Dinner's at 7. Don't be late again üòÑ Mum's making lasagna!                                                                         |ham  |\n","|You‚Äôve been selected for a guaranteed cash prize of ¬£2000! To claim, call 09061701461 NOW!                                          |spam |\n","|Got to the hotel safely. Weather‚Äôs great. Wish you were here!                                                                       |ham  |\n","|FreeMsg: CLAIM YOUR FREE RINGTONE NOW! Just text the word ‚ÄòTONE‚Äô to 87131. Don‚Äôt miss out!                                          |spam |\n","|Happy birthday! üéâ Hope today is full of good vibes and cake.                                                                       |ham  |\n","|URGENT! Your mobile number has won ¬£5000 cash. Call 09066362231 to claim your prize. T&C apply.                                     |spam |\n","+------------------------------------------------------------------------------------------------------------------------------------+-----+\n","\n"]}],"source":["result.select(\n","    F.explode(F.arrays_zip(result.document.result, result.class_.result)).alias(\"cols\")\n",").select(\n","    F.expr(\"cols['0']\").alias(\"document\"), F.expr(\"cols['1']\").alias(\"class\")\n",").show(\n","    truncate=False\n",")"]},{"cell_type":"markdown","metadata":{"id":"oMcI7Bxx8tNf"},"source":["It seems that the spam classifier is working well on these examples! Except for the second message, which should be labeled as `spam` instead of `ham`.\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.11"}},"nbformat":4,"nbformat_minor":0}