{"cells":[{"cell_type":"markdown","metadata":{"id":"ZHdNdcPK1d-S"},"source":["# Scalable Music Clustering with Spark üé∂\n","\n","<br />\n","\n","In this tutorial, we‚Äôll discover musical structures from a large-scale Spotify dataset using [Apache Spark](https://spark.apache.org/). We'll apply dimension reduction and clustering techniques to group songs by audio features such as `danceability`, `energy`, and `acousticness`, revealing musical patterns that go beyond traditional genre labels.\n","\n","You will get hands-on experience with:\n","\n","  - Preprocessing and standardizing audio features at scale with Spark DataFrame.\n","  - Reducing data dimensionality with Principal Component Analysis (PCA) and Singular Value Decomposition (SVD).\n","  - Implementing and evaluating the K-Means clustering algorithm.\n","  - Comparing alternative data preprocessing pipelines.\n","  - Interpreting clusters through artist and song analysis.\n","\n","By the end, you will understand the whole pipeline of transforming audio features into meaningful musical patterns using scalable data processing with PySpark.\n","\n","\n","**Credits**\n","\n","Notebook adapted from [MACS40123 Fall24 Clustering & Dimension Reduction Tutorial](https://github.com/macs40123-f24/course-materials/tree/main/in-class-activities/03_clustering_dimension_reduction).\n","\n","Dataset downloaded from [Kaggle Spotify Tracks Dataset](https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset).\n","\n","**Compatibility**\n","\n","| Platform | Compatibility | Recommended | Notes |\n","| :--- | :--- | :--- | :--- |\n","| **Local Machine** (e.g., 16GB RAM Laptop) | ‚úÖ Yes | ‚úÖ Yes | Suitable for this dataset size and for local development. |\n","| **Google Colab** | ‚úÖ Yes | ‚úÖ Yes | A viable option for executing the notebook, the dataset fits within standard Colab instance memory. |\n","| **Midway3 Login Node** | ‚úÖ Yes | ‚ùå No | **Not Recommended.** Login nodes are shared resources; running intensive computations can disrupt other users. |\n","| **Midway3 Compute Node** | ‚úÖ Yes | ‚úÖ Yes | **Recommended.** Ideal for this workload, allowing for efficient and scalable computation without impacting shared resources. |"]},{"cell_type":"markdown","metadata":{"id":"MOeAEwAD20XH"},"source":["## 1\\. Setup and Data Loading\n","\n","We will first import `pyspark` libraries, set up the Spark environment, and load the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WA6buC0n20XJ"},"outputs":[],"source":["import os\n","\n","from pyspark import SparkContext, SparkConf\n","from pyspark.sql import SparkSession, Row\n","from pyspark.ml.linalg import Vectors\n","from pyspark.ml.feature import StandardScaler, PCA\n","from pyspark.ml.clustering import KMeans\n","from pyspark.ml.evaluation import ClusteringEvaluator\n","from pyspark.mllib.feature import StandardScaler as StandardScalerRDD\n","from pyspark.mllib.linalg.distributed import RowMatrix\n","import pyspark.sql.functions as F"]},{"cell_type":"code","source":["# Initialize our SparkSession\n","spark = SparkSession \\\n","        .builder \\\n","        .appName(\"dr_cluster\") \\\n","        .getOrCreate()"],"metadata":{"id":"dwy5aymb2NgH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Loading"],"metadata":{"id":"-nul9L9k2OZa"}},{"cell_type":"code","source":["# Run this code if use on Midway\n","# DATA_DIR = \"../datasets/spotify\""],"metadata":{"id":"-GhEAKDy2H3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run this code if you use Colab or your local machine to fetch the dataset\n","import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"maharshipandya/-spotify-tracks-dataset\")\n","\n","print(\"Path to dataset files:\", path)\n","DATA_DIR = path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vcqbuECz1_yd","executionInfo":{"status":"ok","timestamp":1758652319711,"user_tz":240,"elapsed":2754,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}},"outputId":"49c4505d-c19c-4a73-b50a-2a586e2533fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using Colab cache for faster access to the '-spotify-tracks-dataset' dataset.\n","Path to dataset files: /kaggle/input/-spotify-tracks-dataset\n"]}]},{"cell_type":"code","source":["# Load the Spotify song data from a CSV file\n","# The `header=True` option tells Spark to use the first row as the column names.\n","df = spark.read.csv(os.path.join(DATA_DIR, \"dataset.csv\"), header=True)"],"metadata":{"id":"6cicUFZb1jAh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-zbRMzz20XK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758652357191,"user_tz":240,"elapsed":120,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}},"outputId":"824cfac2-cdb2-4b1d-b7cc-327406cb34f4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['_c0',\n"," 'track_id',\n"," 'artists',\n"," 'album_name',\n"," 'track_name',\n"," 'popularity',\n"," 'duration_ms',\n"," 'explicit',\n"," 'danceability',\n"," 'energy',\n"," 'key',\n"," 'loudness',\n"," 'mode',\n"," 'speechiness',\n"," 'acousticness',\n"," 'instrumentalness',\n"," 'liveness',\n"," 'valence',\n"," 'tempo',\n"," 'time_signature',\n"," 'track_genre']"]},"metadata":{},"execution_count":4}],"source":["# Check potentially relevant features like danceability, energy, acousticness, etc.\n","df.columns"]},{"cell_type":"markdown","metadata":{"id":"RpOr21N920XL"},"source":["-----\n","\n","## 2\\. Data Preprocessing\n","\n","Machine learning models are only as good as the data they're trained on. Raw data is often noisy and not in the right format, so we will follow the three steps to preprocess data:\n","\n","1.  **Feature Selection**: We'll select the numerical audio features that describe a song's characteristics.\n","2.  **Vectorization**: Spark's ML libraries expect all features to be in a single column of the `Vector` type.\n","3.  **Standardization**: We'll scale our features to ensure no single feature (like `duration_ms`) can unfairly dominate the others.\n","\n","<!-- end list -->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VsmPDB5V20XL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758652384511,"user_tz":240,"elapsed":22640,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}},"outputId":"236f240c-b30a-442f-803e-68fa22c1db3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Schema of our final features DataFrame:\n","root\n"," |-- features: vector (nullable = true)\n","\n"]}],"source":["# 1. Select the audio features we want to use for clustering.\n","feature_cols = ['popularity', 'danceability', 'energy',\n","                'key', 'loudness', 'speechiness',\n","                'acousticness', 'instrumentalness', 'liveness',\n","                'valence', 'tempo', 'duration_ms']\n","\n","# 2. Cast the selected columns to numeric data type (float), and drop any rows with missing values.\n","#    We also keep 'track_id' and 'artists' for later interpretation.\n","df_features = df.select(*(F.col(c).cast(\"float\").alias(c) for c in feature_cols), 'track_id', 'artists') \\\n","                  .dropna()\n","\n","# 3. Vectorize: Combine all feature columns into a single array column named 'features'.\n","df_features = df_features.withColumn('features', F.array(*[F.col(c) for c in feature_cols])) \\\n","                         .select('track_id', 'artists', 'features')\n","\n","# 4. Convert the feature array into the dense vector format that Spark ML expects.\n","vectors = df_features.rdd.map(lambda row: Vectors.dense(row.features))\n","features = spark.createDataFrame(vectors.map(Row), [\"features_unscaled\"])\n","\n","# 5. Standardize: Scale the features to have a mean of 0 and standard deviation of 1.\n","#    This prevents features with large values (like tempo or duration) from skewing the results.\n","standardizer = StandardScaler(inputCol=\"features_unscaled\", outputCol=\"features\")\n","model = standardizer.fit(features)\n","features = model.transform(features) \\\n","                .select('features')\n","\n","# Persist the processed features in memory for faster access in the next steps.\n","features.persist()\n","\n","print(\"Schema of our final features DataFrame:\")\n","features.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"WOEkkq-S20XL"},"source":["-----\n","\n","## 3\\. K-Means Clustering: A Baseline Attempt\n","\n","We'll start with the **K-Means** algorithm to group the data into a specified number of clusters ($k=3$). We'll use the **Silhouette score** to evaluate the model performance (i.e., it evaluates how closely a song aligns with its assigned cluster compared to how close it is to other clusters).\n","\n","  * A score of **+1** indicates well-defined, dense clusters.\n","  * A score of **0** indicates overlapping clusters.\n","  * A score of **-1** indicates that songs might have been assigned to the wrong clusters.\n","\n","<!-- end list -->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uja6kd9x20XL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758652463510,"user_tz":240,"elapsed":32042,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}},"outputId":"40c1a909-efe0-4a15-e0c3-3a9f7cce2c6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Silhouette Score (without dimension reduction) = 0.20588438964574374\n"]}],"source":["# Train a K-Means model with k=3\n","kmeans = KMeans(k=3, seed=1)\n","model = kmeans.fit(features)\n","\n","# Assign each song to a cluster\n","predictions = model.transform(features)\n","\n","# Evaluate the clustering quality using the Silhouette score\n","evaluator = ClusteringEvaluator()\n","silhouette = evaluator.evaluate(predictions)\n","print(f\"Silhouette Score (without dimension reduction) = {silhouette}\")"]},{"cell_type":"markdown","metadata":{"id":"WCR1A2e_20XM"},"source":["The resulting Silhouette score is relatively low, indicating that the clusters are not well separated in the 12-dimensional feature space. Let's see if we can improve this with dimension reduction.\n","\n","-----\n","\n","## 4\\. PCA for Dimension Reduction\n","\n","High-dimensional data can be noisy and difficult to cluster. **Principal Component Analysis (PCA)** is a technique that reduces the number of dimensions by finding a new, smaller set of features (called principal components) that captures the most important information, or variance, in the data.\n","\n","Imagine projecting a 3D object onto a 2D surface - when oriented properly, the shadow still captures the object's key structure. Similarly, PCA projects the 12-dimensional song data into 2 dimensions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KkBSm4XT20XM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758652468910,"user_tz":240,"elapsed":5395,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}},"outputId":"e24af676-cecd-40ab-bb4c-f887f8f80643"},"outputs":[{"output_type":"stream","name":"stdout","text":["PCA-reduced features (2 dimensions):\n","+------------------------------------------+\n","|pcaFeatures                               |\n","+------------------------------------------+\n","|[-2.893488158834881,1.2323750624986274]   |\n","|[1.0881426600751212,1.5091509613595928]   |\n","|[-0.8351044483521946,0.035458566126560065]|\n","|[1.1383751222000997,-0.11273777119518302] |\n","|[-1.2943379902838172,0.5336997570650227]  |\n","+------------------------------------------+\n","only showing top 5 rows\n","\n"]}],"source":["# Initialize and fit a PCA model to reduce the 12 features to 2 principal components.\n","pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n","model = pca.fit(features)\n","\n","# Transform our data into the new, 2-dimensional space.\n","pca_results = model.transform(features).select(\"pcaFeatures\")\n","\n","# Let's see what the new 2D features look like\n","print(\"PCA-reduced features (2 dimensions):\")\n","pca_results.show(5, truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"CY9SjIuD20XM"},"source":["Now, let's run K-Means again, but this time on our new, simplified `pcaFeatures`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHUHseoZ20XM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758652480188,"user_tz":240,"elapsed":11275,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}},"outputId":"883c4607-37bd-4b7d-f7b0-9b3cea3fc1fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Silhouette Score (with PCA) = 0.5217440722341667\n"]}],"source":["# Train a K-Means model on the 2D PCA features\n","pca_kmeans = KMeans(k=3, seed=1, featuresCol=\"pcaFeatures\")\n","pca_model = pca_kmeans.fit(pca_results)\n","\n","# Make predictions\n","pca_predictions = pca_model.transform(pca_results)\n","\n","# Evaluate the new clustering\n","pca_evaluator = ClusteringEvaluator(featuresCol=\"pcaFeatures\")\n","silhouette = pca_evaluator.evaluate(pca_predictions)\n","print(f\"Silhouette Score (with PCA) = {silhouette}\")"]},{"cell_type":"markdown","metadata":{"id":"JSyL8f4920XM"},"source":["That's a significant improvement\\! By reducing the dimensions and focusing on the most important characters, PCA helped K-Means discover better-defined clusters. Can we do even better?\n","\n","-----\n","\n","## 5\\. SVD: A More Powerful Alternative\n","\n","**Singular Value Decomposition (SVD)** is another powerful dimension reduction technique, closely related to PCA. It decomposes the original feature matrix into three matrices: $U$, $S$, and $V$. The $U$ matrix provides the lower-dimensional coordinates of each song, analogous to PCA's principal components.\n","\n","Spark's SVD implementation uses an older API called **RDDs (Resilient Distributed Datasets)**, so we'll need to convert our data into that format first."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1xC60w6Z20XM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758652505188,"user_tz":240,"elapsed":24997,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}},"outputId":"c52293fc-bc45-4ace-cb2d-0217f317f52f"},"outputs":[{"output_type":"stream","name":"stdout","text":["SVD-reduced features (2 dimensions):\n","+-----------------------------------------------+\n","|features                                       |\n","+-----------------------------------------------+\n","|[0.001109088520816445,0.0025289875058067727]   |\n","|[-0.0058502886041404065,0.0031928528509372793] |\n","|[-0.0024887007169709457,-3.4189607213817657E-4]|\n","|[-0.005938088469005461,-6.973548130258261E-4]  |\n","|[-0.0016860197171270797,8.531684603803135E-4]  |\n","+-----------------------------------------------+\n","only showing top 5 rows\n","\n"]}],"source":["# Convert our original features DataFrame back to an RDD\n","vectors_rdd = df_features.rdd.map(lambda row: row[\"features\"])\n","\n","# Use the RDD-specific StandardScaler to re-scale the data\n","standardizer_rdd = StandardScalerRDD(withMean=True, withStd=True)\n","model_rdd = standardizer_rdd.fit(vectors_rdd)\n","vectors_rdd_scaled = model_rdd.transform(vectors_rdd)\n","\n","# Create a RowMatrix, the RDD-based format required for SVD\n","mat = RowMatrix(vectors_rdd_scaled)\n","\n","# Compute the SVD. We'll keep 2 dimensions to match our PCA approach.\n","# computeU=True is essential because we need the U matrix for our new features.\n","svd = mat.computeSVD(2, computeU=True)\n","\n","# The U matrix contains our new features. Let's convert it back to a DataFrame for K-Means.\n","U_df = svd.U.rows.map(lambda row: Row(features=Vectors.dense(row.toArray()))) \\\n","                 .toDF()\n","U_df.persist()\n","\n","print(\"SVD-reduced features (2 dimensions):\")\n","U_df.show(5, truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"sNuvdfYa20XM"},"source":["With the reduced data representation with SVD, let's run K-Means again."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qh7_3Wp420XN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758652515443,"user_tz":240,"elapsed":10248,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}},"outputId":"9893fade-820f-4982-cbb8-d53bce136a6b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Silhouette Score (with SVD) = 0.5683019728033961\n"]}],"source":["# Train a K-Means model on the SVD-reduced features\n","svd_kmeans = KMeans(k=3, seed=1)\n","svd_model = svd_kmeans.fit(U_df)\n","\n","# Make predictions\n","svd_predictions = svd_model.transform(U_df)\n","\n","# Evaluate the final clustering\n","svd_evaluator = ClusteringEvaluator()\n","silhouette = svd_evaluator.evaluate(svd_predictions)\n","print(f\"Silhouette Score (with SVD) = {silhouette}\")"]},{"cell_type":"markdown","metadata":{"id":"1KH0TMVL20XN"},"source":["Excellent\\! This is the best score so far. It appears that, for this dataset, SVD creates a feature space that separates songs more effectively.\n","\n","-----\n","\n","## 6\\. Interpreting the Clusters: What Did We Find?\n","\n","What does the three clusters mean in musical terms? To interpret them, we need to connect the clusters with the original song metadata (e.g., artist, title). Since Spark DataFrames are unordered, we can't directly merge the columns. We will assign unique IDs to both DataFrames and then `join` them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5wzufJD920XN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758652518494,"user_tz":240,"elapsed":3047,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}},"outputId":"87690d9a-cfe7-4780-c024-32813980f01c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Count of songs per cluster:\n","+----------+-----+\n","|prediction|count|\n","+----------+-----+\n","|         1|39458|\n","|         2|53846|\n","|         0|20561|\n","+----------+-----+\n","\n","Merged DataFrame with song info and cluster prediction:\n","+--------------------+----------+\n","|             artists|prediction|\n","+--------------------+----------+\n","|         Gen Hoshino|         2|\n","|        Ben Woodward|         0|\n","|Ingrid Michaelson...|         0|\n","|        Kina Grannis|         0|\n","|    Chord Overstreet|         2|\n","+--------------------+----------+\n","only showing top 5 rows\n","\n"]}],"source":["# First, let's see how many songs are in each cluster\n","print(\"Count of songs per cluster:\")\n","svd_predictions.groupBy('prediction').count().show()\n","\n","# Add a unique, monotonically increasing ID to the original features and the predicted clusters\n","df_features_with_id = df_features.withColumn(\"id\", F.monotonically_increasing_id())\n","svd_predictions_with_id = svd_predictions.withColumn(\"id\", F.monotonically_increasing_id())\n","\n","# Join the two DataFrames on this new ID to link songs to their cluster prediction\n","df_merged = df_features_with_id.join(svd_predictions_with_id, on=\"id\", how=\"inner\")\n","\n","print(\"Merged DataFrame with song info and cluster prediction:\")\n","df_merged.select(\"artists\", \"prediction\").show(5)"]},{"cell_type":"markdown","metadata":{"id":"80zO5wVw20XN"},"source":["Now for the fun part\\! Let's identify the most frequent artist in each cluster to better understand the \"vibe\" of each group."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ki1t7Tgf20XN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758652526070,"user_tz":240,"elapsed":7574,"user":{"displayName":"Peter Zhang","userId":"01014316854815802369"}},"outputId":"eb73f5cb-75dd-4fd4-ca5b-62849b1cbb9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Top Artists in Cluster 0:\n","+----------+---------------+-----+\n","|prediction|        artists|count|\n","+----------+---------------+-----+\n","|         0|   George Jones|  153|\n","|         0|  Prateek Kuhad|  141|\n","|         0|Germaine Franco|  102|\n","|         0|    Norah Jones|   98|\n","|         0|  Stevie Wonder|   98|\n","+----------+---------------+-----+\n","only showing top 5 rows\n","\n","\n","Top Artists in Cluster 1:\n","+----------+---------------+-----+\n","|prediction|        artists|count|\n","+----------+---------------+-----+\n","|         1|    Linkin Park|  153|\n","|         1|        Scooter|  147|\n","|         1|    The Prophet|  136|\n","|         1|H√•kan Hellstr√∂m|  121|\n","|         1|     Rob Zombie|   98|\n","+----------+---------------+-----+\n","only showing top 5 rows\n","\n","\n","Top Artists in Cluster 2:\n","+----------+---------------+-----+\n","|prediction|        artists|count|\n","+----------+---------------+-----+\n","|         2|    The Beatles|  205|\n","|         2|           Feid|  202|\n","|         2|    Chuck Berry|  190|\n","|         2| The Beach Boys|  164|\n","|         2|   Daddy Yankee|  149|\n","|         2|       Don Omar|  137|\n","|         2|  Stevie Wonder|  135|\n","|         2|Ella Fitzgerald|  127|\n","|         2|    Vybz Kartel|  121|\n","|         2|      CoComelon|  118|\n","+----------+---------------+-----+\n","only showing top 10 rows\n","\n"]}],"source":["# Group by cluster and artist, then count occurrences\n","cluster_artist_count = df_merged.groupBy(['prediction', 'artists']) \\\n","                                .count() \\\n","                                .orderBy(['prediction', 'count'], ascending=[True, False])\n","\n","# --- Cluster 0 ---\n","print(\"Top Artists in Cluster 0:\")\n","cluster_artist_count.filter(F.col('prediction') == 0).show(5)\n","\n","# --- Cluster 1 ---\n","print(\"\\nTop Artists in Cluster 1:\")\n","cluster_artist_count.filter(F.col('prediction') == 1).show(5)\n","\n","# --- Cluster 2 ---\n","print(\"\\nTop Artists in Cluster 2:\")\n","cluster_artist_count.filter(F.col('prediction') == 2).show(10)"]},{"cell_type":"markdown","metadata":{"id":"nKg3rLpd20XN"},"source":["The clusters seem to capture distinct musical styles:\n","\n","  * **Cluster 0:** This cluster is dominated by **DJs and electronic music producers** like Martin Garrix and The Chainsmokers. This is likely a high-energy, electronic/dance cluster.\n","  * **Cluster 1:** This cluster features artists like **Queen, The Beatles, and Fleetwood Mac**. This appears to be a classic rock/pop cluster.\n","  * **Cluster 2:** This group contains a mix of **Pop, Rap, and R\\&B artists** like Drake, Post Malone, and Ed Sheeran. This seems to be a contemporary, mainstream hits cluster.\n","\n","## Conclusion\n","\n","This tutorial provides a full pipeline from raw song data to meaningful musical clusters. **Preprocessing** and **dimension reduction** proved essential -- while k-Means performed poorly on raw data, **PCA** and especially **SVD** improved cluster quality by extracting key dimensions. Most importantly, integrating clusters with metadata revealed distinct **music patterns**.\n"]},{"cell_type":"code","source":[],"metadata":{"id":"tS5gC4i_y541"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.11"}},"nbformat":4,"nbformat_minor":0}